{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pickle  # For loading serialized Python objects\n",
    "import numpy as np  # Numerical computations\n",
    "import pandas as pd  # Data manipulation\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "import warnings  # For managing warnings\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV  # Cross-validation and hyperparameter tuning\n",
    "from sklearn.preprocessing import StandardScaler  # Feature scaling\n",
    "from sklearn.impute import SimpleImputer  # Missing value imputation\n",
    "from sklearn.feature_selection import SelectKBest, f_classif  # Feature selection using ANOVA F-test\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, confusion_matrix, roc_curve, auc,\n",
    "    f1_score, precision_score, recall_score, hamming_loss, roc_auc_score\n",
    ")\n",
    "from sklearn.base import BaseEstimator, TransformerMixin  # Custom transformers\n",
    "from sklearn.utils import shuffle, compute_class_weight  # Utilities\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTENC  # SMOTE for mixed data types\n",
    "from imblearn.under_sampling import RandomUnderSampler  # Random undersampling\n",
    "\n",
    "# Model and explainability\n",
    "from catboost import CatBoostClassifier, Pool  # Gradient boosting model\n",
    "import shap  # SHAP values for model interpretation\n",
    "\n",
    "# System utilities\n",
    "import os  # Directory and path handling\n",
    "import traceback  # Traceback printing for exception handling\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === Custom transformer to drop columns with all NaNs ===\n",
    "class DropAllNaNColumns(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        self.non_nan_cols_ = ~np.all(X_df.isna(), axis=0)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        return X_df.loc[:, self.non_nan_cols_].values\n",
    "\n",
    "# === Load EEG and cohort data ===\n",
    "file_path = \"/Volumes/NOC_Drive/databases/biomarker_data.pkl\"\n",
    "with open(file_path, \"rb\") as file:\n",
    "    hbn_data = pickle.load(file)\n",
    "\n",
    "# Extract EEG feature data and cohort metadata\n",
    "eeg_data = hbn_data[\"HBN_electrode_space\"][\"eeg_xdata\"]\n",
    "cohort_df = hbn_data[\"HBN_electrode_space\"][\"cohort_df\"].copy()\n",
    "\n",
    "# Extract subject IDs and condition (eyes open/closed) from filename\n",
    "cohort_df['subject_uid'] = cohort_df['eeg_filename'].str.extract(r'HBN-sub-(NDAR[A-Z0-9]+)')[0]\n",
    "cohort_df['condition'] = cohort_df['eeg_filename'].str.extract(r'_(EO|EC)_')[0]\n",
    "\n",
    "# Identify binary disorder columns (only 2 unique values)\n",
    "disorder_cols = [col for col in cohort_df.columns if cohort_df[col].nunique() == 2 and cohort_df[col].dtype in ['int64', 'float64']]\n",
    "print(\"Disorders found:\", disorder_cols)\n",
    "\n",
    "# List of disorder pairs to compare\n",
    "disorder_pairs = [\n",
    "    ('ASD', 'ADHD-Combined Type'),\n",
    "    ('Healthy controls', 'Anxiety Disorders'),\n",
    "    ('SLD (Reading)', 'SLD (Mathematics)'),\n",
    "    ('OCD', 'OCD-Other'),\n",
    "    ('Communication Disorders', 'Intellectual Disability'),\n",
    "    ('Depressive Disorders', 'Trauma and Stressor Related Disorders'),\n",
    "    ('ADHD-Inattentive Type', 'ADHD-Hyperactive/Impulsive Type'),\n",
    "    ('Oppositional Defiant Disorder', 'Disruptive, Impulse Control and Conduct Disorders-Other'),\n",
    "    ('Tic Disorders', 'ASD'),\n",
    "    ('ADHD-Combined Type', 'SLD (Written Expression)'),\n",
    "    ('Anxiety Disorders', 'Depressive Disorders'),\n",
    "    ('Elimination Disorders', 'OCD')\n",
    "]\n",
    "\n",
    "# Keep only those pairs where both disorders exist in the data\n",
    "available_disorders = set(disorder_cols)\n",
    "disorder_pairs = [(d1, d2) for d1, d2 in disorder_pairs if d1 in available_disorders and d2 in available_disorders]\n",
    "print(f\"Evaluating {len(disorder_pairs)} valid disorder pairs...\")\n",
    "\n",
    "# Create directories for results\n",
    "output_dir = \"./disorder_pair_results\"\n",
    "shap_dir = os.path.join(output_dir, \"shap_values\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(shap_dir, exist_ok=True)\n",
    "\n",
    "# Mapping from subject-condition pair to index in EEG data\n",
    "ids = eeg_data.coords['subject_uid'].values\n",
    "conditions = eeg_data.coords['condition'].values\n",
    "index_map = {(uid, cond): i for i, (uid, cond) in enumerate(zip(ids, conditions))}\n",
    "\n",
    "# Helper function to map dataframe rows to EEG data indices\n",
    "def map_indices(df_subset):\n",
    "    indices = []\n",
    "    for row in df_subset.itertuples():\n",
    "        idx = index_map.get((row.subject_uid, row.condition))\n",
    "        if idx is not None:\n",
    "            indices.append(idx)\n",
    "    return indices\n",
    "\n",
    "# Initialize global tracking variables\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "all_pred_probs = []\n",
    "all_results = []\n",
    "\n",
    "# === Main loop over disorder pairs ===\n",
    "for i, (disorder1, disorder2) in enumerate(disorder_pairs, 1):\n",
    "    print(f\"\\n========== Processing disorder pair {i}/{len(disorder_pairs)}: {disorder1} vs {disorder2} ==========\")\n",
    "    try:\n",
    "        # Select only those rows with exclusive labels for each disorder\n",
    "        df_binary = cohort_df[\n",
    "            ((cohort_df[disorder1] == 1) & (cohort_df[disorder2] == 0)) |\n",
    "            ((cohort_df[disorder1] == 0) & (cohort_df[disorder2] == 1))\n",
    "        ].copy()\n",
    "\n",
    "        # Skip if too few samples\n",
    "        if df_binary.shape[0] < 10:\n",
    "            print(f\"Skipping pair {disorder1}-{disorder2}: Not enough samples ({df_binary.shape[0]})\")\n",
    "            continue\n",
    "\n",
    "        # Label is 1 if disorder2, 0 if disorder1\n",
    "        df_binary['label'] = df_binary[disorder2].values\n",
    "        indices = map_indices(df_binary)\n",
    "\n",
    "        # Skip if no corresponding EEG data found\n",
    "        if len(indices) == 0:\n",
    "            print(f\"Skipping pair {disorder1}-{disorder2}: No matching EEG data indices found\")\n",
    "            continue\n",
    "\n",
    "        # Subset EEG data\n",
    "        eeg_selected = eeg_data.isel(ID=indices)\n",
    "        n_samples, n_sources, n_freqs, n_biomarkers = eeg_selected.shape\n",
    "\n",
    "        # Generate feature names\n",
    "        eeg_feature_names_all = [\n",
    "            f\"biomarker_{b}_freq_{f}_source_{s}\"\n",
    "            for s in range(n_sources)\n",
    "            for f in range(n_freqs)\n",
    "            for b in range(n_biomarkers)\n",
    "        ]\n",
    "\n",
    "        # Reshape EEG tensor to 2D array\n",
    "        X_raw = eeg_selected.values.transpose(0, 3, 2, 1).reshape(len(indices), -1)\n",
    "\n",
    "        # Prepare feature dataframe\n",
    "        condition_cat = df_binary['condition'].astype('category').reset_index(drop=True)\n",
    "        X_raw_df = pd.DataFrame(X_raw)\n",
    "        X_df = pd.concat([X_raw_df, condition_cat], axis=1)\n",
    "        X_df.columns = list(map(str, range(X_raw.shape[1]))) + ['condition']\n",
    "        y = df_binary['label'].values\n",
    "        groups = df_binary['subject_uid'].values\n",
    "\n",
    "        # Preprocessing\n",
    "        dropper = DropAllNaNColumns()\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        scaler = StandardScaler()\n",
    "        outer_cv = GroupKFold(n_splits=10)  # Ensures subject independence\n",
    "\n",
    "        # Hyperparameter grid and metrics setup\n",
    "        param_grid = {'depth': [4, 6], 'learning_rate': [0.01, 0.05], 'l2_leaf_reg': [3, 5]}\n",
    "        cat_features = ['condition']\n",
    "        roc_curves = []\n",
    "        fold_metrics = {k: [] for k in ['fold', 'accuracy', 'f1_micro', 'f1_macro', 'precision_macro', 'recall_macro', 'hamming_loss', 'auc']}\n",
    "\n",
    "        # === Cross-validation loop ===\n",
    "        for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X_df, y, groups=groups), 1):\n",
    "            print(f\"\\n--- Outer Fold {fold} ---\")\n",
    "\n",
    "            # Train-test split\n",
    "            X_train = X_df.iloc[train_idx].copy()\n",
    "            y_train = y[train_idx]\n",
    "            X_test = X_df.iloc[test_idx].copy()\n",
    "            y_test = y[test_idx]\n",
    "\n",
    "            # Separate and preprocess EEG data\n",
    "            X_train_eeg = X_train.drop(columns=cat_features).values\n",
    "            X_train_cat = X_train[cat_features].reset_index(drop=True)\n",
    "            X_test_cat = X_test[cat_features].reset_index(drop=True)\n",
    "\n",
    "            # Clean EEG features\n",
    "            X_train_eeg = dropper.fit_transform(X_train_eeg)\n",
    "            X_train_eeg = imputer.fit_transform(X_train_eeg)\n",
    "\n",
    "            # Select top EEG features\n",
    "            k_features = min(int(0.2 * X_train_eeg.shape[1]), 1000)\n",
    "            selector = SelectKBest(score_func=f_classif, k=k_features)\n",
    "            X_train_eeg = selector.fit_transform(X_train_eeg, y_train)\n",
    "            selected_indices = selector.get_support(indices=True)\n",
    "            selected_feature_names = [eeg_feature_names_all[i] for i in selected_indices]\n",
    "            X_train_eeg = scaler.fit_transform(X_train_eeg)\n",
    "\n",
    "            # Reconstruct training DataFrame for CatBoost\n",
    "            X_train_bal_df = pd.DataFrame(X_train_eeg, columns=[f'feat_{i}' for i in range(X_train_eeg.shape[1])])\n",
    "            X_train_bal_cat = pd.DataFrame(X_train_cat.values[np.random.choice(X_train_cat.shape[0], len(X_train_eeg), replace=True)],\n",
    "                                           columns=X_train_cat.columns).reset_index(drop=True)\n",
    "            X_train_bal_df = pd.concat([X_train_bal_df.reset_index(drop=True), X_train_bal_cat], axis=1)\n",
    "            cat_idx = [X_train_bal_df.columns.get_loc(col) for col in cat_features]\n",
    "\n",
    "            # Balance classes with SMOTENC and undersampling\n",
    "            desired_ratio = 0.6\n",
    "            n_majority = np.sum(y_train == 0)\n",
    "            n_minority = np.sum(y_train == 1)\n",
    "            target_minority = int(n_majority * desired_ratio)\n",
    "            if n_minority < target_minority and n_minority > 1:\n",
    "                smote_nc = SMOTENC(categorical_features=cat_idx, sampling_strategy={1: target_minority},\n",
    "                                   random_state=fold, k_neighbors=min(5, n_minority - 1))\n",
    "                X_train_bal_arr, y_train_bal = smote_nc.fit_resample(X_train_bal_df.values, y_train)\n",
    "            else:\n",
    "                X_train_bal_arr, y_train_bal = X_train_bal_df.values, y_train\n",
    "\n",
    "            # Undersample majority class\n",
    "            rus = RandomUnderSampler(sampling_strategy='auto', random_state=fold)\n",
    "            X_train_bal_arr, y_train_bal = rus.fit_resample(X_train_bal_arr, y_train_bal)\n",
    "            X_train_bal_df = pd.DataFrame(X_train_bal_arr, columns=X_train_bal_df.columns)\n",
    "\n",
    "            # Process test set similarly\n",
    "            X_test_eeg = X_test.drop(columns=cat_features).values\n",
    "            X_test_eeg = dropper.transform(X_test_eeg)\n",
    "            X_test_eeg = imputer.transform(X_test_eeg)\n",
    "            X_test_eeg = selector.transform(X_test_eeg)\n",
    "            X_test_eeg = scaler.transform(X_test_eeg)\n",
    "            X_test_ready = pd.DataFrame(X_test_eeg, columns=[f'feat_{i}' for i in range(X_test_eeg.shape[1])])\n",
    "            X_test_ready = pd.concat([X_test_ready.reset_index(drop=True), X_test_cat], axis=1)\n",
    "\n",
    "            # Set categorical columns\n",
    "            for df in [X_train_bal_df, X_test_ready]:\n",
    "                for col in cat_features:\n",
    "                    df[col] = df[col].astype('category')\n",
    "\n",
    "            # Compute class weights for imbalance\n",
    "            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_bal), y=y_train_bal)\n",
    "            class_weights_dict = dict(zip(np.unique(y_train_bal), class_weights))\n",
    "\n",
    "            # Train model with grid search\n",
    "            base_clf = CatBoostClassifier(iterations=100, eval_metric='F1:use_weights=true',\n",
    "                                          random_seed=42, verbose=0, early_stopping_rounds=30,\n",
    "                                          class_weights=class_weights_dict)\n",
    "            grid = GridSearchCV(base_clf, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "            grid.fit(X_train_bal_df, y_train_bal, cat_features=cat_features)\n",
    "            best_model = grid.best_estimator_\n",
    "\n",
    "            # Evaluate on test set\n",
    "            train_pool = Pool(X_train_bal_df, y_train_bal, cat_features=cat_features)\n",
    "            test_pool = Pool(X_test_ready, y_test, cat_features=cat_features)\n",
    "            best_model.fit(train_pool, eval_set=test_pool, use_best_model=True)\n",
    "            y_proba = best_model.predict_proba(test_pool)[:, 1]\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "            roc_curves.append((fpr, tpr))\n",
    "            optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "            y_pred = (y_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "            # Save metrics\n",
    "            all_true_labels.extend(y_test)\n",
    "            all_pred_labels.extend(y_pred)\n",
    "            all_pred_probs.extend(y_proba)\n",
    "            fold_metrics['fold'].append(fold)\n",
    "            fold_metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "            fold_metrics['f1_micro'].append(f1_score(y_test, y_pred, average='micro'))\n",
    "            fold_metrics['f1_macro'].append(f1_score(y_test, y_pred, average='macro'))\n",
    "            fold_metrics['precision_macro'].append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "            fold_metrics['recall_macro'].append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "            fold_metrics['hamming_loss'].append(hamming_loss(y_test, y_pred))\n",
    "            fold_metrics['auc'].append(auc(fpr, tpr))\n",
    "\n",
    "            print(f\"Fold {fold} metrics for {disorder1} vs {disorder2}:\")\n",
    "            print(f\"  Accuracy: {fold_metrics['accuracy'][-1]:.4f}\")\n",
    "            print(f\"  F1 Micro: {fold_metrics['f1_micro'][-1]:.4f}\")\n",
    "            print(f\"  F1 Macro: {fold_metrics['f1_macro'][-1]:.4f}\")\n",
    "            print(f\"  Precision Macro: {fold_metrics['precision_macro'][-1]:.4f}\")\n",
    "            print(f\"  Recall Macro: {fold_metrics['recall_macro'][-1]:.4f}\")\n",
    "            print(f\"  Hamming Loss: {fold_metrics['hamming_loss'][-1]:.4f}\")\n",
    "            print(f\"  AUC: {fold_metrics['auc'][-1]:.4f}\")\n",
    "\n",
    "            # Save SHAP values for interpretability\n",
    "            explainer = shap.TreeExplainer(best_model)\n",
    "            shap_values = explainer.shap_values(train_pool)\n",
    "            shap_df = pd.DataFrame(shap_values, columns=selected_feature_names + cat_features)\n",
    "            shap_df['label'] = y_train_bal\n",
    "            shap_df.to_csv(os.path.join(shap_dir, f\"shap_{disorder1.replace(' ', '_')}_vs_{disorder2.replace(' ', '_')}_fold{fold}.csv\"), index=False)\n",
    "\n",
    "        # Aggregate and save metrics for this pair\n",
    "        pair_metrics = {\n",
    "            'disorder1': disorder1,\n",
    "            'disorder2': disorder2,\n",
    "            'accuracy_mean': np.mean(fold_metrics['accuracy']),\n",
    "            'f1_micro_mean': np.mean(fold_metrics['f1_micro']),\n",
    "            'f1_macro_mean': np.mean(fold_metrics['f1_macro']),\n",
    "            'precision_macro_mean': np.mean(fold_metrics['precision_macro']),\n",
    "            'recall_macro_mean': np.mean(fold_metrics['recall_macro']),\n",
    "            'hamming_loss_mean': np.mean(fold_metrics['hamming_loss']),\n",
    "            'auc_mean': np.mean(fold_metrics['auc'])\n",
    "        }\n",
    "        all_results.append(pair_metrics)\n",
    "        pd.DataFrame([pair_metrics]).to_csv(f\"{output_dir}/metrics_{disorder1}_vs_{disorder2}.csv\", index=False)\n",
    "        pd.DataFrame(fold_metrics).to_csv(f\"{output_dir}/fold_metrics_{disorder1}_vs_{disorder2}.csv\", index=False)\n",
    "\n",
    "        # Plot fold metrics\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.lineplot(data=pd.DataFrame(fold_metrics).set_index('fold'))\n",
    "        plt.title(f\"Fold-wise Metrics for {disorder1} vs {disorder2}\")\n",
    "        plt.xlabel(\"Fold\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/fold_metrics_plot_{disorder1}_vs_{disorder2}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {disorder1} vs {disorder2}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Save summary of all disorder pair metrics\n",
    "summary_df = pd.DataFrame(all_results)\n",
    "summary_df.to_csv(f\"{output_dir}/all_disorder_pairs_metrics_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\nProcessing completed for all disorder pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to compute average SHAP importances per condition (disorder) in a binary classification pair\n",
    "def average_shap_importances_per_disorder(pair_name, shap_dir):\n",
    "    # Get list of all SHAP value files for the specified disorder pair\n",
    "    fold_files = [f for f in os.listdir(shap_dir)\n",
    "                  if f.startswith(f\"shap_{pair_name}_fold\") and f.endswith(\".csv\")]\n",
    "    fold_files.sort()  # Ensure consistent ordering of folds\n",
    "\n",
    "    # Initialize nested dictionary:\n",
    "    # disorder_feature_shap[condition][feature] = list of SHAP values across samples\n",
    "    disorder_feature_shap = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Iterate over each SHAP file (one per fold)\n",
    "    for file in fold_files:\n",
    "        shap_df = pd.read_csv(os.path.join(shap_dir, file))  # Load SHAP values\n",
    "        feature_cols = [col for col in shap_df.columns if col not in ['condition', 'label']]  # Get feature columns\n",
    "\n",
    "        # Iterate through each row (sample) in the SHAP dataframe\n",
    "        for _, row in shap_df.iterrows():\n",
    "            disorder = row['condition']  # Get the condition ('EO' or 'EC', or other categorical indicator)\n",
    "            for col in feature_cols:\n",
    "                try:\n",
    "                    shap_val = abs(row[col])  # Use absolute SHAP value for importance\n",
    "                    disorder_feature_shap[disorder][col].append(shap_val)\n",
    "                except:\n",
    "                    continue  # Skip any problematic columns (e.g., missing values)\n",
    "\n",
    "    # Compute average SHAP value and rank features by importance for each condition\n",
    "    disorder_avg_ranks = {}\n",
    "    for disorder, feat_dict in disorder_feature_shap.items():\n",
    "        # Calculate mean SHAP value for each feature\n",
    "        avg_importance = {feat: np.mean(vals) for feat, vals in feat_dict.items()}\n",
    "        # Sort features by average importance in descending order\n",
    "        ranked_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        # Store the ranked features per disorder\n",
    "        disorder_avg_ranks[disorder] = ranked_features\n",
    "\n",
    "    return disorder_avg_ranks\n",
    "\n",
    "# Example usage with a specific disorder pair\n",
    "pair_name = \"ADHD-Combined_Type_vs_SLD_(Written_Expression)\"  # Filename-compatible version of disorder pair\n",
    "shap_dir = \"/Users/tuanadurmayuksel/Desktop/Internship_code/hbn-dataset-special-data/HBN /Final/montage/disorder_pair_results/shap_values/Shap\"\n",
    "\n",
    "# Run the function to get average feature importance per disorder\n",
    "result = average_shap_importances_per_disorder(pair_name, shap_dir)\n",
    "\n",
    "# Optional: Convert the result to a DataFrame for easier analysis or visualization\n",
    "def result_to_df(disorder_avg_ranks):\n",
    "    all_rows = []\n",
    "    for disorder, feats in disorder_avg_ranks.items():\n",
    "        for rank, (feat, avg_val) in enumerate(feats, 1):\n",
    "            all_rows.append({'Feature': feat, 'Avg_SHAP': avg_val, 'Rank': rank})\n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "# Convert the SHAP results to a DataFrame\n",
    "df_result = result_to_df(result)\n",
    "\n",
    "# Display top 20 most important features across disorders\n",
    "print(df_result.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping lists\n",
    "biomarkers = ['AbsolutePower', 'RelativePower', 'DFA', 'fEI']\n",
    "\n",
    "frequencies = [\n",
    "    '1.0-4.0 Hz', '4.0-5.1 Hz', '5.1-6.5 Hz', '6.5-8.3 Hz', '8.3-10.5 Hz',\n",
    "    '10.5-13.4 Hz', '13.4-17.0 Hz', '17.0-21.7 Hz', '21.7-27.6 Hz',\n",
    "    '27.6-35.2 Hz', '35.2-44.8 Hz'\n",
    "]\n",
    "\n",
    "sources = [\n",
    "    'Cz','E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10', 'E11', 'E12', 'E13',\n",
    "    'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E20', 'E21', 'E22', 'E23', 'E24', 'E25',\n",
    "    'E26', 'E27', 'E28', 'E29', 'E30', 'E31', 'E32', 'E33', 'E34', 'E35', 'E36', 'E37',\n",
    "    'E38', 'E39', 'E40', 'E41', 'E42', 'E43', 'E44', 'E45', 'E46', 'E47', 'E48', 'E49',\n",
    "    'E50', 'E51', 'E52', 'E53', 'E54', 'E55', 'E56', 'E57', 'E58', 'E59', 'E60', 'E61',\n",
    "    'E62', 'E63', 'E64', 'E65', 'E66', 'E67', 'E68', 'E69', 'E70', 'E71', 'E72', 'E73',\n",
    "    'E74', 'E75', 'E76', 'E77', 'E78', 'E79', 'E80', 'E81', 'E82', 'E83', 'E84', 'E85',\n",
    "    'E86', 'E87', 'E88', 'E89', 'E90', 'E91', 'E92', 'E93', 'E94', 'E95', 'E96', 'E97',\n",
    "    'E98', 'E99', 'E100', 'E101', 'E102', 'E103', 'E104', 'E105', 'E106', 'E107',\n",
    "    'E108', 'E109', 'E110', 'E111', 'E112', 'E113', 'E114', 'E115', 'E116', 'E117',\n",
    "    'E118', 'E119', 'E120', 'E121', 'E122', 'E123', 'E124', 'E125', 'E126', 'E127',\n",
    "    'E128', \n",
    "]\n",
    "\n",
    "# Function to parse and map a feature name\n",
    "def parse_feature_name(feature):\n",
    "    try:\n",
    "        parts = feature.split('_')\n",
    "        freq_idx = int(parts[1])\n",
    "        biomarker_idx = int(parts[3])\n",
    "        source_idx = int(parts[5])\n",
    "        return {\n",
    "            'Feature': feature,\n",
    "            'Frequency': frequencies[freq_idx] if freq_idx < len(frequencies) else 'Unknown',\n",
    "            'Biomarker': biomarkers[biomarker_idx] if biomarker_idx < len(biomarkers) else 'Unknown',\n",
    "            'Source': sources[source_idx] if source_idx < len(sources) else 'Unknown'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'Feature': feature,\n",
    "            'Frequency': 'Error',\n",
    "            'Biomarker': 'Error',\n",
    "            'Source': 'Error'\n",
    "        }\n",
    "\n",
    "# Map features in result DataFrame\n",
    "def map_feature_descriptions(df_result, top_n=20):\n",
    "    mapped = [parse_feature_name(feat) for feat in df_result['Feature'].head(top_n)]\n",
    "    df_mapped = pd.DataFrame(mapped)\n",
    "    df_mapped['Avg_SHAP'] = df_result['Avg_SHAP'].head(top_n).values\n",
    "    df_mapped['Rank'] = df_result['Rank'].head(top_n).values\n",
    "    return df_mapped\n",
    "\n",
    "# Example usage\n",
    "df_mapped_result = map_feature_descriptions(df_result, top_n=20)\n",
    "print(df_mapped_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your combined metrics CSV\n",
    "file_path = \"/Users/tuanadurmayuksel/Desktop/Internship_code/hbn-dataset-special-data/HBN /Final/montage/disorder_pair_results/all_disorder_pairs_metrics_summary.csv\"\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "file_path = \"/Users/tuanadurmayuksel/Desktop/Internship_code/hbn-dataset-special-data/HBN /Final/montage/disorder_pair_results/all_disorder_pairs_metrics_summary.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Disorder name mapping\n",
    "disorder_name_map = {\n",
    "    'ASD': 'ASD', 'ADHD-Combined Type': 'ADHD-C', 'Healthy controls': 'Healthy',\n",
    "    'Anxiety Disorders': 'Anxiety', 'SLD (Reading)': 'SLD-Read', 'SLD (Mathematics)': 'SLD-Math',\n",
    "    'SLD (Written Expression)': 'SLD-Write', 'OCD': 'OCD', 'OCD-Other': 'OCD-Other',\n",
    "    'Communication Disorders': 'CommDis', 'Intellectual Disability': 'ID',\n",
    "    'Depressive Disorders': 'Depression', 'Trauma and Stressor Related Disorders': 'Trauma-Stress',\n",
    "    'Oppositional Defiant Disorder': 'ODD', \n",
    "    'Disruptive, Impulse Control and Conduct Disorders-Other': 'Impulse',\n",
    "    'Tic Disorders': 'Tic', 'Elimination Disorders': 'Elimination',\n",
    "    'ADHD-Hyperactive/Impulsive Type': 'ADHD-H', 'ADHD-Inattentive Type': 'ADHD-I'\n",
    "}\n",
    "\n",
    "# Create pair names\n",
    "df['pair'] = df['disorder1'].map(disorder_name_map) + ' vs ' + df['disorder2'].map(disorder_name_map)\n",
    "\n",
    "# Define metrics\n",
    "metrics = ['accuracy_mean', 'f1_micro_mean', 'f1_macro_mean', \n",
    "           'precision_macro_mean', 'recall_macro_mean', 'auc_mean']\n",
    "pretty_metrics = ['Accuracy', 'F1 Micro', 'F1 Macro', 'Precision', 'Recall', 'AUC']\n",
    "\n",
    "# Prepare heatmap data\n",
    "heatmap_data = df.set_index('pair')[metrics]\n",
    "heatmap_data.columns = pretty_metrics\n",
    "heatmap_data = heatmap_data.apply(lambda col: col.fillna(col.mean()), axis=0)\n",
    "\n",
    "# Define desired order manually\n",
    "desired_order = [\n",
    "    \"CommDis vs ID\",\n",
    "    \"Elimination vs OCD\",\n",
    "    \"ASD vs ADHD-C\",\n",
    "    \"ODD vs Impulse\",\n",
    "    \"Depression vs Trauma-Stress\",\n",
    "    \"Anxiety vs Depression\",\n",
    "    \"Tic vs ASD\",\n",
    "    \"SLD-Read vs SLD-Math\",\n",
    "    \"ADHD-C vs SLD-Write\",\n",
    "    \"OCD vs OCD-Other\",\n",
    "    \"Healthy vs Anxiety\"\n",
    "]\n",
    "\n",
    "# Filter and reorder DataFrame\n",
    "heatmap_data = heatmap_data.loc[desired_order]\n",
    "\n",
    "# Numbered labels for legend\n",
    "numbered_labels = [f\"{i+1}. {pair}\" for i, pair in enumerate(desired_order)]\n",
    "\n",
    "# Replace index with numbers\n",
    "heatmap_data.index = list(range(1, len(desired_order) + 1))\n",
    "\n",
    "# Use a light blue gradient\n",
    "light_green_cmap = sns.light_palette(\"#66BB6A\", as_cmap=True)  # A soft pastel green\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=light_green_cmap,\n",
    "    annot_kws={\"size\": 30, \"weight\": \"bold\", \"color\": \"black\"},\n",
    "    cbar_kws={'label': 'Metric Value'},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Axis label formatting\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=15, fontsize=30, color='black')\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=30,  color='black')\n",
    "ax.set_xlabel('Metrics', fontsize=30, fontweight='bold', color='black')\n",
    "ax.set_ylabel('Disorder Pairs (Numbered)', fontsize=30, fontweight='bold', color='black')\n",
    "\n",
    "# Colorbar formatting\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label('Metric Value', fontsize=30, color='black',fontweight='bold')\n",
    "cbar.ax.tick_params(labelsize=30)\n",
    "\n",
    "# Adjust layout to add label text at bottom\n",
    "plt.subplots_adjust(bottom=0.35)\n",
    "\n",
    "# Add disorder pair legend below plot\n",
    "fig.text(0.5, 0.01, \"\\n\".join(numbered_labels), ha='center', va='top', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the metric columns to visualize\n",
    "metrics_cols = [\n",
    "    'accuracy_mean', 'f1_micro_mean', 'f1_macro_mean',\n",
    "    'precision_macro_mean', 'recall_macro_mean', 'auc_mean'\n",
    "]\n",
    "# Friendly labels for the radar chart axes\n",
    "pretty_labels = ['Accuracy', 'F1 Micro', 'F1 Macro', 'Precision', 'Recall', 'AUC']\n",
    "\n",
    "# Extract the relevant data from your DataFrame\n",
    "data = df[metrics_cols]\n",
    "\n",
    "# Calculate the mean and standard deviation for each metric across folds/samples\n",
    "avg_metrics = data.mean()\n",
    "std_metrics = data.std()\n",
    "\n",
    "# Prepare arrays for plotting\n",
    "metrics = avg_metrics.values              # Mean values for each metric\n",
    "stds = std_metrics.values                 # Standard deviations for each metric\n",
    "labels = pretty_labels                    # Axis labels\n",
    "num_vars = len(labels)                    # Number of axes\n",
    "\n",
    "# Compute the angle for each axis (equal spacing around the circle)\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]  # Append first angle to close the loop\n",
    "\n",
    "# Close the data loops for plotting\n",
    "metrics = np.append(metrics, metrics[0])\n",
    "stds = np.append(stds, stds[0])\n",
    "\n",
    "# Calculate upper and lower bounds for the std deviation band\n",
    "upper = metrics + stds\n",
    "lower = metrics - stds\n",
    "\n",
    "# Define benchmark cases for comparison on the radar chart\n",
    "best_case = np.ones(num_vars + 1)        # Perfect case: all metrics = 1\n",
    "worst_case = np.zeros(num_vars + 1)      # Worst case: all metrics = 0\n",
    "avg_case = np.full(num_vars + 1, 0.5)    # Average baseline: all metrics = 0.5\n",
    "\n",
    "# Initialize polar plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True), dpi=160)\n",
    "\n",
    "# Define colors and transparency levels\n",
    "color_mean = \"#2E8B57\"        # SeaGreen for the mean line/fill\n",
    "color_fill = \"#2E8B57\"\n",
    "alpha_main = 0.99             # Almost opaque mean fill\n",
    "alpha_std = 0.25              # Light transparency for std deviation band\n",
    "label_color = \"#2F4F4F\"       # DarkSlateGray (not used here but ready)\n",
    "\n",
    "color_best = \"#1f77b4\"        # Blue for best case line\n",
    "color_worst = \"#d62728\"       # Red for worst case markers\n",
    "color_avg_case = \"#ff7f0e\"    # Orange for average case line\n",
    "\n",
    "# Plot mean metric line and fill area\n",
    "ax.plot(angles, metrics, color=color_mean, linewidth=2.5, label=\"Mean\")\n",
    "ax.fill(angles, metrics, color=color_fill, alpha=alpha_main)\n",
    "\n",
    "# Shade ±1 standard deviation around the mean\n",
    "ax.fill_between(angles, lower, upper, color=color_fill, alpha=alpha_std, label=\"± 1 Std Dev\")\n",
    "\n",
    "# Plot best case as a dashed blue line\n",
    "ax.plot(angles, best_case, color=color_best, linewidth=2, linestyle='--', label=\"Best Case (1.0)\")\n",
    "\n",
    "# Plot worst case as red dots at zero radius on each axis\n",
    "ax.scatter(angles[:-1], worst_case[:-1], color=color_worst, s=100, marker='o', label=\"Worst Case (0.0)\")\n",
    "\n",
    "# Plot average baseline as a dashed orange line\n",
    "ax.plot(angles, avg_case, color=color_avg_case, linewidth=2, linestyle='--', label=\"Average Case (0.5)\")\n",
    "\n",
    "# Configure axis labels with large, bold font\n",
    "ax.set_thetagrids(np.degrees(angles[:-1]), labels, fontsize=30, fontweight='semibold', color=\"black\")\n",
    "\n",
    "# Set radial ticks and labels from 0 to 1, spaced evenly\n",
    "radial_ticks = np.linspace(0.0, 1.0, 6)  # [0.0, 0.2, ..., 1.0]\n",
    "ax.set_yticks(radial_ticks)\n",
    "ax.set_yticklabels([f\"{x:.1f}\" for x in radial_ticks], fontsize=20, color='black')\n",
    "ax.yaxis.grid(True, color='black', linestyle='--', alpha=0.7, linewidth=1)\n",
    "ax.set_rlim(0, 1.0)  # Set radial axis limits\n",
    "\n",
    "# Adjust plot rotation and direction for better readability\n",
    "ax.set_theta_offset(np.pi / 2)  # Start from top\n",
    "ax.set_theta_direction(-1)      # Clockwise ordering\n",
    "\n",
    "# Hide the circular frame and set background color\n",
    "ax.spines[\"polar\"].set_visible(False)\n",
    "ax.set_facecolor(\"white\")\n",
    "\n",
    "# Annotate mean metric values near each axis for clarity\n",
    "for angle, value in zip(angles[:-1], metrics[:-1]):\n",
    "    # Align text depending on angle quadrant to avoid overlap\n",
    "    if 0 <= angle <= np.pi / 2:\n",
    "        ha, va, dy = 'left', 'bottom', 0.04\n",
    "    elif np.pi / 2 < angle <= np.pi:\n",
    "        ha, va, dy = 'right', 'bottom', 0.04\n",
    "    elif np.pi < angle <= 3*np.pi/2:\n",
    "        ha, va, dy = 'right', 'top', -0.04\n",
    "    else:\n",
    "        ha, va, dy = 'left', 'top', -0.04\n",
    "\n",
    "    ax.text(angle, value + dy, f\"{value:.2f}\",\n",
    "            ha=ha, va=va, fontsize=20, fontweight='medium', color='black',\n",
    "            bbox=dict(boxstyle='round,pad=0.25', facecolor='white', edgecolor=color_mean, linewidth=0.8, alpha=0.85))\n",
    "\n",
    "# Add legend outside the plot to the right\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.4, 1.2), fontsize=14)\n",
    "\n",
    "# Optimize layout to prevent clipping\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# --- CONFIG ---\n",
    "shap_dir = \"/Users/tuanadurmayuksel/Desktop/Internship_code/hbn-dataset-special-data/HBN /Final/montage/disorder_pair_results/shap_values/Shap\"\n",
    "shap_files = [f for f in os.listdir(shap_dir) if f.startswith(\"shap_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# --- HELPER: extract disorder pair name ---\n",
    "def extract_pair_name(filename):\n",
    "    match = re.match(r\"shap_(.*?)_vs_(.*?)_fold\\d+\\.csv\", filename)\n",
    "    if match:\n",
    "        return f\"{match.group(1).replace('_', ' ')} vs {match.group(2).replace('_', ' ')}\"\n",
    "    return None\n",
    "\n",
    "# --- STEP 1: Aggregate SHAP values per disorder pair ---\n",
    "pair_shap_dict = {}\n",
    "\n",
    "for file in shap_files:\n",
    "    filepath = os.path.join(shap_dir, file)\n",
    "    pair_name = extract_pair_name(file)\n",
    "    \n",
    "    if pair_name is None:\n",
    "        continue  # skip files that don't match pattern\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    shap_vals = df.drop(columns=['label'])\n",
    "    mean_abs_shap = shap_vals.abs().mean(axis=0)\n",
    "    \n",
    "    if pair_name not in pair_shap_dict:\n",
    "        pair_shap_dict[pair_name] = []\n",
    "    pair_shap_dict[pair_name].append(mean_abs_shap)\n",
    "\n",
    "# --- STEP 1.5: Collect all features across all pairs ---\n",
    "all_features = set()\n",
    "for shap_list in pair_shap_dict.values():\n",
    "    for mean_abs_shap in shap_list:\n",
    "        all_features.update(mean_abs_shap.index)\n",
    "\n",
    "all_features = sorted(list(all_features))\n",
    "\n",
    "# --- STEP 2: Final SHAP matrix per disorder pair (mean across folds, aligned features) ---\n",
    "pair_names = []\n",
    "shap_matrix = []\n",
    "\n",
    "for pair, shap_list in pair_shap_dict.items():\n",
    "    pair_names.append(pair)\n",
    "    \n",
    "    # Reindex each fold's mean vector to include all features, fill missing with 0\n",
    "    reindexed = [s.reindex(all_features, fill_value=0) for s in shap_list]\n",
    "    \n",
    "    # Concatenate along columns and average across folds\n",
    "    mean_vector = pd.concat(reindexed, axis=1).mean(axis=1)\n",
    "    shap_matrix.append(mean_vector.values)\n",
    "\n",
    "shap_matrix = np.vstack(shap_matrix)\n",
    "\n",
    "# --- STEP 3: Apply MDS ---\n",
    "mds = MDS(n_components=2, random_state=42)\n",
    "coords = mds.fit_transform(shap_matrix)\n",
    "coords_scaled = coords * 2.5  # optional scaling\n",
    "\n",
    "# --- STEP 4: Define disorder short name map ---\n",
    "disorder_name_map = {\n",
    "    'ASD': 'ASD', 'ADHD-Combined Type': 'ADHD-C', 'Healthy controls': 'Healthy',\n",
    "    'Anxiety Disorders': 'Anxiety', 'SLD (Reading)': 'SLD-Read', 'SLD (Mathematics)': 'SLD-Math',\n",
    "    'SLD (Written Expression)': 'SLD-Write', 'OCD': 'OCD', 'OCD-Other': 'OCD-Other',\n",
    "    'Communication Disorders': 'CommDis', 'Intellectual Disability': 'ID',\n",
    "    'Depressive Disorders': 'Depression', 'Trauma and Stressor Related Disorders': 'Trauma-Stress',\n",
    "    'Oppositional Defiant Disorder': 'ODD', \n",
    "    'Disruptive, Impulse Control and Conduct Disorders-Other': 'Impulse',\n",
    "    'Tic Disorders': 'Tic', 'Elimination Disorders': 'Elimination',\n",
    "    'ADHD-Hyperactive/Impulsive Type': 'ADHD-H', 'ADHD-Inattentive Type': 'ADHD-I'\n",
    "}\n",
    "\n",
    "# --- STEP 5: Plot MDS with short disorder names ---\n",
    "fig1, ax1 = plt.subplots(figsize=(12, 8), dpi=100)\n",
    "ax1.scatter(coords_scaled[:, 0], coords_scaled[:, 1], c='lightgray', s=100, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "def get_short_name(full_name):\n",
    "    return disorder_name_map.get(full_name, full_name)\n",
    "\n",
    "# offset = 0.01\n",
    "# offset1 = 0.005\n",
    "\n",
    "for i, (x, y) in enumerate(coords_scaled):\n",
    "    disorder1, disorder2 = pair_names[i].split(\" vs \")\n",
    "    short_label = f\"{get_short_name(disorder1)} vs {get_short_name(disorder2)}\"\n",
    "    ax1.text(x+0.005 , y+0.02 , short_label, fontsize=13, fontweight='bold', ha='center', va='center', color='black')\n",
    "\n",
    "ax1.set_xlabel(\"MDS Dimension 1\", fontsize=16)\n",
    "ax1.set_ylabel(\"MDS Dimension 2\", fontsize=16)\n",
    "ax1.grid(True, linestyle='--', alpha=0.3)\n",
    "ax1.tick_params(labelsize=20)\n",
    "ax1.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 5: Plot MDS with bigger dots and numbers, better axis labels, and numbered legend below ---\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(12, 11), dpi=100)  # extra height for labels below\n",
    "\n",
    "dot_size = 1500  # bigger dots\n",
    "number_fontsize = 30  # bigger numbers on dots\n",
    "legend_fontsize = 20  # bigger labels below\n",
    "\n",
    "ax1.scatter(coords_scaled[:, 0], coords_scaled[:, 1], c='lightgrey', s=dot_size, edgecolors='black', linewidth=0.7)\n",
    "\n",
    "# Number each dot with bigger, bold text centered on the dots\n",
    "for i, (x, y) in enumerate(coords_scaled):\n",
    "    ax1.text(x, y, str(i+1), fontsize=number_fontsize, fontweight='bold', ha='center', va='center', color='black')\n",
    "\n",
    "# Better axis labels\n",
    "ax1.set_xlabel(\"MDS Dimension 1 \", fontsize=30, fontweight='bold')\n",
    "ax1.set_ylabel(\"MDS Dimension 2 \", fontsize=30, fontweight='bold')\n",
    "\n",
    "ax1.grid(True, linestyle='--', alpha=0.3)\n",
    "ax1.tick_params(labelsize=14)\n",
    "ax1.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "# Prepare numbered disorder pair list with bigger font, separated by newlines\n",
    "numbered_labels = [f\"{i+1}. {get_short_name(pair.split(' vs ')[0])} vs {get_short_name(pair.split(' vs ')[1])}\" \n",
    "                   for i, pair in enumerate(pair_names)]\n",
    "\n",
    "# Adjust bottom margin to fit labels\n",
    "plt.subplots_adjust(bottom=0.35)\n",
    "\n",
    "# Place labels below the plot, centered and spaced nicely\n",
    "fig1.text(0.5, 0.001, \"\\n\".join(numbered_labels), va='top', fontsize=legend_fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# --- Configuration parameters ---\n",
    "threshold = 0.01                      # Minimum normalized SHAP value to visualize\n",
    "montage_type = 'GSN-HydroCel-128'    # EEG sensor layout to use for topomap plotting\n",
    "figsize = (2, 2)                     # Size of each individual topomap subplot\n",
    "font_size = 8                        # Font size for subplot titles and labels\n",
    "max_cols = 6                        # Max number of columns in the subplot grid\n",
    "max_rows = 5                        # Max number of rows in the subplot grid\n",
    "\n",
    "# Map feature names to human-readable labels and filter for positive SHAP values only\n",
    "df_mapped_result = map_feature_descriptions(df_result, top_n=1000)\n",
    "df = df_mapped_result.copy()\n",
    "df = df[df['Avg_SHAP'] > 0]          # Keep only features with positive average SHAP\n",
    "\n",
    "# Prepare EEG channel info and 2D sensor positions for topomap plotting\n",
    "montage = mne.channels.make_standard_montage(montage_type)\n",
    "ch_names = montage.ch_names\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=256, ch_types='eeg')\n",
    "info.set_montage(montage)\n",
    "pos = np.array([info['chs'][i]['loc'][:2] for i in range(len(info['chs']))])  # 2D sensor coords\n",
    "\n",
    "# Group data by Frequency and Biomarker to plot separate topomaps for each combination\n",
    "combinations = list(df.groupby(['Frequency', 'Biomarker']))\n",
    "combinations = combinations[:max_cols * max_rows]  # Limit to max plots that fit grid\n",
    "\n",
    "n_plots = len(combinations)\n",
    "n_cols = min(max_cols, n_plots)                 # Calculate grid columns based on data & max allowed\n",
    "n_rows = min(max_rows, int(np.ceil(n_plots / n_cols)))  # Calculate rows to fit all plots\n",
    "\n",
    "# Compute overall figure size based on grid layout and individual subplot size\n",
    "fig_width = figsize[0] * n_cols\n",
    "fig_height = figsize[1] * n_rows\n",
    "\n",
    "# Create a figure with a grid of subplots (polar topomaps)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(fig_width, fig_height))\n",
    "axes = axes.flatten() if n_plots > 1 else [axes]  # Flatten axes array for easy iteration\n",
    "\n",
    "cmap = plt.cm.bwr  # Blue-white-red colormap for visualizing positive/negative values (here just positive)\n",
    "\n",
    "# Loop over each Frequency-Biomarker group to compute and plot topomap of SHAP importance\n",
    "for i, ((freq, biom), group) in enumerate(combinations):\n",
    "    # Initialize dictionary with 0 importance for all EEG channels\n",
    "    importance_map = {ch: 0 for ch in ch_names}\n",
    "    \n",
    "    # Aggregate Avg_SHAP values per channel/source for the current group\n",
    "    for _, row in group.iterrows():\n",
    "        if row['Source'] in importance_map:\n",
    "            importance_map[row['Source']] += row['Avg_SHAP']\n",
    "\n",
    "    # Normalize importance values to [0,1] for coloring on the topomap\n",
    "    values = np.array(list(importance_map.values()))\n",
    "    norm_values = (values - values.min()) / (values.max() - values.min() + 1e-8)\n",
    "\n",
    "    # Apply threshold: values below threshold are set to zero (not visualized)\n",
    "    norm_map = {ch: (val if val > threshold else 0) for ch, val in zip(ch_names, norm_values)}\n",
    "    data = np.array([norm_map[ch] for ch in ch_names])\n",
    "\n",
    "    # Plot the topomap on the corresponding subplot axis\n",
    "    ax = axes[i]\n",
    "    im, _ = mne.viz.plot_topomap(data, pos, axes=ax, show=False, contours=0, cmap=cmap)\n",
    "    ax.set_title(f\"{freq}\\n{biom}\", fontsize=font_size)  # Title with frequency and biomarker\n",
    "    ax.axis('off')  # Hide axis ticks\n",
    "\n",
    "# Turn off axes for any unused subplot slots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Add a vertical colorbar to the right of the subplots for importance scale\n",
    "cbar_ax = fig.add_axes([0.93, 0.2, 0.015, 0.6])\n",
    "fig.colorbar(im, cax=cbar_ax, label='Normalized Importance')\n",
    "\n",
    "# Add an overall title for the figure\n",
    "plt.suptitle(\"EEG SHAP Importance by Frequency & Biomarker\", fontsize=font_size + 2)\n",
    "\n",
    "# Adjust layout to fit subplot titles and colorbar nicely\n",
    "plt.tight_layout(rect=[0, 0, 0.92, 1])\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "# Display the complete figure with all topomap subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode all features, not just top 20\n",
    "mapped_all = [parse_feature_name(feat) for feat in df_result['Feature']]\n",
    "shap_to_original = pd.DataFrame(mapped_all)\n",
    "shap_to_original['importance'] = df_result['Avg_SHAP']  # Match with SHAP values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Aggregate mean importance values by Biomarker and Frequency, reshape to wide format\n",
    "heatmap_data = shap_to_original.groupby(['Biomarker', 'Frequency'])['importance'].mean().unstack()\n",
    "\n",
    "# Check if all values are zero to avoid meaningless plot\n",
    "if np.max(heatmap_data.values) == 0:\n",
    "    print(\"All coefficient values are zero! Cannot plot meaningful heatmap.\")\n",
    "else:\n",
    "    # Normalize data by dividing by max value for color scaling between 0 and 1\n",
    "    heatmap_data_norm = heatmap_data / np.max(heatmap_data.values)\n",
    "\n",
    "    # Clean column names by stripping whitespace and rename specific frequency bands\n",
    "    heatmap_data_norm.columns = heatmap_data_norm.columns.str.strip()\n",
    "    heatmap_data_norm = heatmap_data_norm.rename(columns={\n",
    "        'RelativePower': 'RP',\n",
    "        'AbsolutePower': 'AP'\n",
    "    })\n",
    "\n",
    "    # Clean index (Biomarker) names similarly and rename if needed\n",
    "    heatmap_data_norm.index = heatmap_data_norm.index.str.strip()\n",
    "    heatmap_data_norm = heatmap_data_norm.rename(index={\n",
    "        'RelativePower': 'RP',\n",
    "        'AbsolutePower': 'AP'\n",
    "        # Add more biomarker renames here if necessary\n",
    "    })\n",
    "\n",
    "    # Create the heatmap figure with specified size\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot heatmap with color mapping, annotations, and styling\n",
    "    ax = sns.heatmap(\n",
    "        heatmap_data_norm,\n",
    "        cmap=\"YlGnBu\",\n",
    "        cbar_kws={'aspect': 35},     # Aspect ratio for colorbar\n",
    "        annot=True,                  # Show values on heatmap\n",
    "        fmt=\".2f\",                  # Format annotation to 2 decimal places\n",
    "        annot_kws={\"size\": 20},     # Font size for annotations\n",
    "        linecolor='black',           # Grid line color between cells\n",
    "        linewidth=1                  # Grid line width\n",
    "    )\n",
    "\n",
    "    # Set axis labels with font sizes and rotations\n",
    "    plt.xlabel(\"Frequency Band\", fontsize=18)\n",
    "    plt.ylabel(\"\", fontsize=18)  # Leave y-axis label blank for custom label\n",
    "\n",
    "    plt.xticks(fontsize=16, rotation=25, fontweight='bold')\n",
    "    plt.yticks(fontsize=16, rotation=0, fontweight='bold')\n",
    "\n",
    "    # Remove left spine for cleaner look\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    # Add custom rotated label \"Biomarker\" to y-axis with positioning\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "    ax.text(\n",
    "        x=-0.10, y=0.40, s=\"Biomarker\",\n",
    "        fontsize=18, rotation=90,\n",
    "        ha='center', va='bottom',\n",
    "        transform=ax.transAxes\n",
    "    )\n",
    "\n",
    "    # Customize colorbar ticks and label font size\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "    cbar.set_label('Normalized SHAP Importance', fontsize=20)\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the heatmap plot\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
